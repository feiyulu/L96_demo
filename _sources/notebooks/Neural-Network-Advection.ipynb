{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural networks to parameterize advection in L96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from L96_model import (\n",
    "    L96,\n",
    "    RK2,\n",
    "    RK4,\n",
    "    EulerFwd,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    ")\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# In this exercise we will be looking at conservation properties in L96, which requires a high-order numerical scheme.\n",
    "time_method = RK4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We are only going to use the single equation model from {cite}`Lorenz1995`, or equation 3.1:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F\n",
    "\\end{align}\n",
    "\n",
    "The reason we do this is because the advection term has a much larger control on the stability of the system than the scale-interaction term.  It is fairly difficult to learn a model for the sub-grid scale term that causes L96 to go unstable so long as the timestep is sufficient to keep the advection term stable.\n",
    "\n",
    "We still want to to look into the stability of a learned parameterization, but to explore the stability in more detail we are going to focus on learning a neural-network for the advection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a 1d and 2d version of the single-equation L96 model:\n",
    "\n",
    "The '1d' in time, or advectionless version of L96 reduces to:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "= - X_k + F,\n",
    "\\end{align}\n",
    "the steady state solution is simply:\n",
    "\n",
    "\\begin{align}\n",
    "X_k=F,\n",
    "\\end{align}\n",
    "\n",
    "and the time-dependent solution is an exponential:\n",
    "\n",
    "\\begin{align}\n",
    "X_k\n",
    "= \\left(F- (F-X_k^0)\\exp(-t) \\right).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We are going to generate both 2d (w/ advection) and 1d (w/o advection) versions of the L96 model.  The 2d model will then be used as training data to build a non-local neural network that can reproduce the effect of including the advection term.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - This is a standard GCM class including a polynomial parameterization in rhs of equation for tendency.\n",
    "#  In this experiment we will not be using the parameterization in this class but have left it for generality.\n",
    "class GCM:\n",
    "    def __init__(self, F, parameterization, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F) + self.parameterization(param, X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - This is the same as the GCM with one notable exception.\n",
    "#  We have set the advection flag to False in the RHS of the L96 equation.\n",
    "class GCM_1d:\n",
    "    def __init__(self, F, parameterization, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F, advect=False) + self.parameterization(\n",
    "            param, X, self.F\n",
    "        )\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample configuration\n",
    "\n",
    "First we will run the 2d and 1d version of the model with a modest forcing of $F=10$.\n",
    "\n",
    "We are going to try to simulate the effect of climate model drift on parameter space by running the same model but with $F=20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose a modest forcing and simulate for 100 cycles\n",
    "Forcing, dt, T = 10, 0.01, 100\n",
    "Forcing_x10 = 20  # Forcing*10\n",
    "\n",
    "# Choose an random set of initial conditions\n",
    "b = 10\n",
    "init_cond = b * np.random.randn(8)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# We create the template 2d GCM here with the polynomial parameterization\n",
    "# this model will be used to generate training data to learn the advection term.\n",
    "naive_parameterization = lambda param, X, F: np.polyval(param, X)\n",
    "gcm_2d = GCM(Forcing, naive_parameterization)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# We also create a super GCM for simulation with the forcing of 100.\n",
    "# This will be used as the truth when we test the ability of the 1d model with the neural network to\n",
    "# work outside of the parmameter space it was trained.\n",
    "gcm_2d_x10 = GCM(Forcing_x10, naive_parameterization)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Finally,we build the 1d GCM including the polynomial parameterization,\n",
    "# and we create the corresponding super GCM with forcing squared.\n",
    "gcm_1d = GCM_1d(Forcing, naive_parameterization)\n",
    "gcm_1d_x10 = GCM_1d(Forcing_x10, naive_parameterization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the 2d and 1d versions of the GCM and GCM with F=100 (\"_x10\")\n",
    "\n",
    "x2d, t2d = gcm_2d(\n",
    "    init_cond,\n",
    "    dt,\n",
    "    int(T / dt),\n",
    "    [\n",
    "        0.0,\n",
    "    ],\n",
    ")\n",
    "x2d_x10, t2d_x10 = gcm_2d_x10(\n",
    "    init_cond,\n",
    "    dt,\n",
    "    int(T / dt),\n",
    "    [\n",
    "        0.0,\n",
    "    ],\n",
    ")\n",
    "\n",
    "x1d, t1d = gcm_1d(\n",
    "    init_cond,\n",
    "    dt,\n",
    "    int(T / dt),\n",
    "    [\n",
    "        0.0,\n",
    "    ],\n",
    ")\n",
    "x1d_x10, t1d_x10 = gcm_1d_x10(\n",
    "    init_cond,\n",
    "    dt,\n",
    "    int(T / dt),\n",
    "    [\n",
    "        0.0,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics:\n",
    "\n",
    "We are going to track the momentum and energy of L96 via the following metrics:\n",
    "\n",
    "### Momentum:\n",
    "\\begin{align}\n",
    "p = \\sum_k X_k\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Energy:\n",
    "\\begin{align}\n",
    "e = \\sum_k X_k^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "These metrics are chosen to track the system.  We are looking for a conservative property of the L96 system.  It turns out in the single equation form of the L96 problem one of these two metrics is conserved by the advection process, which is the energy like term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in [2, T]:\n",
    "    F, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    a = ax.ravel()[0]\n",
    "    a.plot(t1d, np.sum(x1d, axis=1), label=\"1d\", color=\"r\", linewidth=2)\n",
    "    a.plot(t2d, np.sum(x2d, axis=1), label=\"2d\", color=\"b\", linewidth=2)\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "    a.set_title(\"L96 momentum\")\n",
    "    a.set_xlabel(\"t\")\n",
    "    a.set_ylabel(r\"$\\sum_k X_k$\")\n",
    "    a.set_xlim(0, tup)\n",
    "\n",
    "    a = ax.ravel()[1]\n",
    "    a.plot(t1d, np.sum(x1d**2, axis=1), label=\"1d\", color=\"r\", linewidth=2)\n",
    "    a.plot(t2d, np.sum(x2d**2, axis=1), label=\"2d\", color=\"b\", linewidth=2)\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "    a.set_title(\"L96 energy\")\n",
    "    a.set_xlabel(\"t\")\n",
    "    a.set_ylabel(r\"$\\sum_k X_k^2$\")\n",
    "    a.set_xlim(0, tup)\n",
    "\n",
    "    F.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Conservation of energy in L96\n",
    "\n",
    "To demonstrate the conservation of energy in L96 advection we build a model with 0 forcing and 0 damping.\n",
    "\n",
    "Note that the cyan line is an experiment only undergoing forcing by the advection term.  The momentum is clearly not conserved, but the energy is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the forcing\n",
    "Forcing_demo = 0\n",
    "# Zero the damping via a linear parameterization term:\n",
    "P_nodamp = [1.0, 0.0]\n",
    "\n",
    "# Running the 2d and 1d versions of the GCM and GCM with squared forcing (\"s\")\n",
    "\n",
    "gcm_2d_demo = GCM(Forcing_demo, naive_parameterization)\n",
    "# The parameterization here is countering the decay term to demonstrate the conservation of this system\n",
    "x2d_demo, t2d_demo = gcm_2d_demo(init_cond, dt, int(T / dt), P_nodamp)\n",
    "\n",
    "\n",
    "for tup in [2, T]:\n",
    "    F, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    a = ax.ravel()[0]\n",
    "    a.plot(\n",
    "        t2d_demo,\n",
    "        np.sum(x2d_demo, axis=1),\n",
    "        label=\"2d, F=0 no damp\",\n",
    "        color=\"c\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    a.plot(t2d, np.sum(x2d, axis=1), label=\"2d, F=10\", color=\"b\", linewidth=2)\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "    a.set_title(\"L96 momentum\")\n",
    "    a.set_xlabel(\"t\")\n",
    "    a.set_ylabel(r\"$\\sum_k X_k$\")\n",
    "    a.set_xlim(0, tup)\n",
    "\n",
    "    a = ax.ravel()[1]\n",
    "    a.plot(\n",
    "        t2d_demo,\n",
    "        np.sum(x2d_demo**2, axis=1),\n",
    "        label=\"2d, F=0 no damp\",\n",
    "        color=\"c\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    a.plot(t2d, np.sum(x2d**2, axis=1), label=\"2d, F=10\", color=\"b\", linewidth=2)\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "    a.set_title(\"L96 energy\")\n",
    "    a.set_xlabel(\"t\")\n",
    "    a.set_ylabel(r\"$\\sum_k X_k^2$\")\n",
    "    a.set_xlim(0, tup)\n",
    "\n",
    "    F.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a dataset of advection tendencies to learn\n",
    "\n",
    "In the next section we are going to create a dataset of advection tendencies to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first set of data to learn is built with the standard forcing\n",
    "\n",
    "obs = gcm_2d\n",
    "model = gcm_1d\n",
    "\n",
    "N = 20000\n",
    "\n",
    "X = []\n",
    "Xm1 = []\n",
    "Xm2 = []\n",
    "Xp1 = []\n",
    "Adv = []\n",
    "\n",
    "# randomize the initial condition and run 1000 time-step spin up with the real world model\n",
    "init_condr = 10 * np.random.randn(8)\n",
    "x_2, _ = gcm_2d(\n",
    "    init_condr,\n",
    "    0.01,\n",
    "    1000,\n",
    "    [\n",
    "        0.0,\n",
    "    ],\n",
    ")\n",
    "\n",
    "for ti in range(N):\n",
    "    # Set the initial condition from the spin up/2d model\n",
    "    init_condr_up = x_2[-1, :]\n",
    "\n",
    "    # Real world values\n",
    "    x_2, _ = obs(\n",
    "        init_condr_up,\n",
    "        0.01,\n",
    "        1,\n",
    "        [\n",
    "            0.0,\n",
    "        ],\n",
    "    )\n",
    "    # Simple model values\n",
    "    x_1, _ = model(\n",
    "        init_condr_up,\n",
    "        0.01,\n",
    "        1,\n",
    "        [\n",
    "            0.0,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # This is the difference in the tendency term due to neglecting 2d processes per time-step\n",
    "    Adv.append((x_2[-1, :] - x_1[-1, :]).ravel() / dt)\n",
    "\n",
    "    # Storing the state variable and its rolled forms for plotting and learning convenience\n",
    "    X.append(x_1[-1, :].ravel())\n",
    "    Xm1.append(np.roll(x_1[-1, :], 1).ravel())\n",
    "    Xm2.append(np.roll(x_1[-1, :], 2).ravel())\n",
    "    Xp1.append(np.roll(x_1[-1, :], -1).ravel())\n",
    "\n",
    "X = np.array(X)\n",
    "Xm1 = np.array(Xm1)\n",
    "Xm2 = np.array(Xm2)\n",
    "Xp1 = np.array(Xp1)\n",
    "Adv = np.array(Adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a second set of learning data with the F=20 forcing\n",
    "\n",
    "obs = gcm_2d_x10\n",
    "model = gcm_1d_x10\n",
    "\n",
    "\n",
    "X_x10 = []\n",
    "Xm1_x10 = []\n",
    "Xm2_x10 = []\n",
    "Xp1_x10 = []\n",
    "Adv_x10 = []\n",
    "\n",
    "# randomize the initial condition and run 1000 time-step spin up with the real world model\n",
    "init_condr = 10 * np.random.randn(8)\n",
    "x_2, _ = gcm_2d(\n",
    "    init_condr,\n",
    "    0.01,\n",
    "    1000,\n",
    "    [\n",
    "        0.0,\n",
    "    ],\n",
    ")\n",
    "\n",
    "for ti in range(N):\n",
    "    # Set the initial condition from the spin up/2d model\n",
    "    init_condr_up = x_2[-1, :]\n",
    "\n",
    "    # Real world values\n",
    "    x_2, _ = obs(\n",
    "        init_condr_up,\n",
    "        0.01,\n",
    "        1,\n",
    "        [\n",
    "            0.0,\n",
    "        ],\n",
    "    )\n",
    "    # Simple model values\n",
    "    x_1, _ = model(\n",
    "        init_condr_up,\n",
    "        0.01,\n",
    "        1,\n",
    "        [\n",
    "            0.0,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # This is the difference in the tendency term due to neglecting 2d processes per time-step\n",
    "    Adv_x10.append((x_2[-1, :] - x_1[-1, :]).ravel() / dt)\n",
    "\n",
    "    # Storing the state variable and its rolled forms for plotting and learning convenience\n",
    "    X_x10.append(x_1[-1, :].ravel())\n",
    "    Xm1_x10.append(np.roll(x_1[-1, :], 1).ravel())\n",
    "    Xm2_x10.append(np.roll(x_1[-1, :], 2).ravel())\n",
    "    Xp1_x10.append(np.roll(x_1[-1, :], -1).ravel())\n",
    "\n",
    "X_x10 = np.array(X_x10)\n",
    "Xm1_x10 = np.array(Xm1_x10)\n",
    "Xm2_x10 = np.array(Xm2_x10)\n",
    "Xp1_x10 = np.array(Xp1_x10)\n",
    "Adv_x10 = np.array(Adv_x10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a parameter to parameterize from\n",
    "\n",
    "If we were simply looking at data and knew that the advection term was a missing force, we might start by looking at correlations with $X_k$ values, but we would quickly relize that this is not effective.\n",
    "\n",
    "Even taking part of the actual advection term does not yield a useful feature parameter.\n",
    "\n",
    "In principle we should be able to learn a parameterization with all combinations of polynomials including all $X_k$'s, which should yield something close to the right answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X, Adv, \"k.\")\n",
    "plt.xlabel(\"$X_{k}$\")\n",
    "plt.ylabel(\"Advection\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Xm1, Adv, \"k.\")\n",
    "plt.xlabel(\"$X_{k-1}$\")\n",
    "plt.ylabel(\"Advection\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Xp1, Adv, \"k.\")\n",
    "plt.xlabel(\"$X_{k+1}$\")\n",
    "plt.ylabel(\"Advection\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Xm2 - Xp1, Adv, \"k.\")\n",
    "plt.xlabel(\"$X_{k-2}-X_{k+1}$\")\n",
    "plt.ylabel(\"Advection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's now just assume that we knew the form of the advection term.  We now get something that looks like a 1:1 linear relationship between the observed advection term and the correct feature parameter.  It is not perfect because the values we are using for $X_k$ are not consistent with the RK4 time stepping (if we used forward Euler we would get a perfit fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to assume we know the feature variable that we need to train the model.\n",
    "# However, because of sampling across a time-step we will not fit a perfect 1:1,\n",
    "# we end up with something very close to 1:1, but we will use a higher order polynomial that will\n",
    "# fail when used outside the training data.\n",
    "\n",
    "# First we will tune with the original F=10 output\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "Feature = -Xm1 * (np.array(Xm2) - np.array(Xp1))\n",
    "plt.plot(Feature, Adv, \"r.\")\n",
    "plt.xlabel(\"$X_{k-1}(X_{k-2}-X_{k+1})$\")\n",
    "plt.ylabel(\"Advection tendency\")\n",
    "\n",
    "# This parameterization might fail when used outside of the training data.\n",
    "# Note if we used the Forward Euler timestepping we would get closer to a 1:1 fit for the data.\n",
    "P = np.polyfit(np.array(Feature).ravel(), np.array(Adv).ravel(), 1)\n",
    "print(\"Fit (slope/bias): \", P)\n",
    "FS = [-200, 200]\n",
    "plt.plot(FS, np.polyval(P, FS), \"g-\", label=\"polynomial parameterization\")\n",
    "plt.plot(FS, FS, \"k--\", label=\"1:1\")\n",
    "plt.xlim(FS)\n",
    "plt.ylim(FS)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 1d GCM with a parameterization for the advection based on the known advection parameter\n",
    "advection_parameterization = lambda param, X, F: np.polyval(\n",
    "    param, -np.roll(X, 1) * (np.roll(X, 2) - np.roll(X, -1))\n",
    ")\n",
    "gcm_1d_padv = GCM_1d(Forcing, advection_parameterization)\n",
    "gcm_1d_padv_x10 = GCM_1d(Forcing_x10, advection_parameterization)\n",
    "\n",
    "# Here is the 1d GCM with the learned advection via the linear parameterization\n",
    "xplinear, tplinear = gcm_1d_padv(init_cond, dt, int(T / dt), P)\n",
    "\n",
    "# And the same 1d GCM applied out of sample\n",
    "xplinear_x10, tplinear_x10 = gcm_1d_padv_x10(init_cond, dt, int(T / dt), P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompExps(Exp1, ExpN):\n",
    "    # Exp1 - reference experiment list\n",
    "    # ExpN - list of comparison experiments\n",
    "\n",
    "    try:\n",
    "        T1 = Exp1[0]\n",
    "        X1 = Exp1[1]\n",
    "        L1 = Exp1[2]\n",
    "        F, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        a = ax.ravel()[0]\n",
    "        a.plot(T1, np.sum(X1, axis=1), label=L1, color=\"k\", linewidth=3)\n",
    "        a = ax.ravel()[1]\n",
    "        a.plot(T1, np.sum(X1**2, axis=1), label=L1, color=\"k\", linewidth=3)\n",
    "\n",
    "        F2, ax2 = plt.subplots(1, figsize=(5, 4))\n",
    "\n",
    "        for Exp in ExpN:\n",
    "            TN = Exp[0]\n",
    "            XN = Exp[1]\n",
    "            LN = Exp[2]\n",
    "\n",
    "            a = ax.ravel()[0]\n",
    "            a.plot(TN, np.sum(XN, axis=1), label=LN, linewidth=2)\n",
    "            a = ax.ravel()[1]\n",
    "            a.plot(TN, np.sum(XN**2, axis=1), label=LN, linewidth=2)\n",
    "\n",
    "            _X = []\n",
    "            _Y = []\n",
    "            for ii in range(1, 100, 1):\n",
    "                _X.append(np.percentile(np.sum(X1[int(5 // dt) :] ** 2, axis=1), ii))\n",
    "                _Y.append(np.percentile(np.sum(XN[int(5 // dt) :] ** 2, axis=1), ii))\n",
    "            ax2.plot(_X, _Y, \".-\", label=LN)\n",
    "\n",
    "        for ii in range(2):\n",
    "            a = ax.ravel()[ii]\n",
    "            a.legend()\n",
    "            a.grid(True)\n",
    "        ax[0].set(ylabel=r\"$\\sum_k X_k$\", xlabel=r\"$t$\")\n",
    "        ax[1].set(ylabel=r\"$\\sum_k X_k^2$\", xlabel=r\"$t$\")\n",
    "        F.tight_layout()\n",
    "\n",
    "        ax2.grid(True)\n",
    "        ax2.set(\n",
    "            xlabel=\"e 2d model\",\n",
    "            ylabel=\"e 1d model w/ param\",\n",
    "            title=\"q-q plot of energy in 2d and parameterized model\",\n",
    "        )\n",
    "        LIM = np.nanmax(list(_X) + list(_Y))\n",
    "        ax2.set_xlim(0, LIM * 1.1)\n",
    "        ax2.set_ylim(0, LIM * 1.1)\n",
    "        ax2.plot([0, LIM], [0, LIM], \"y-\")\n",
    "        pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This (should) learn a stable 'parameterization' for the advection that is very close to the real advection term.\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tplinear, xplinear, \"1d w/ linear\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It even extrapolates to the F=20 model\n",
    "\n",
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tplinear_x10, xplinear_x10, \"1d w/ linear\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we chose the wrong feature?\n",
    "\n",
    "It turns out you can find features that are approximately correct and build a decent model for the advection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use a feature that is wrong to train the model\n",
    "\n",
    "plt.figure()\n",
    "Feature = -(np.array(Xm2) - np.array(Xp1) * np.array(Xm1))\n",
    "plt.plot(Feature, Adv, \"r.\")\n",
    "plt.xlabel(\"$(X_{k-1}-X_{k+1})$\")\n",
    "plt.ylabel(\"Advection tendency/Forcing\")\n",
    "\n",
    "P_wrong = np.polyfit(np.array(Feature).ravel(), np.array(Adv).ravel(), 1)\n",
    "FS = np.sort(np.array(Feature).ravel())\n",
    "plt.plot(FS, np.polyval(P_wrong, FS), \"g-\", label=\"polynomial parameterization\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 1d GCM with a 2d parameterization\n",
    "advection_parameterization_wrong = lambda param, X, F: np.polyval(\n",
    "    param, -(np.roll(X, 2) - np.roll(X, -1) * np.roll(X, 1))\n",
    ")\n",
    "gcm_1d_padv_wrong = GCM_1d(Forcing, advection_parameterization_wrong)\n",
    "\n",
    "# Here is the 1d GCM with the learned advection\n",
    "x_wrongp, t_wrongp = gcm_1d_padv_wrong(init_cond, dt, int(T / dt), P_wrong)\n",
    "\n",
    "# This goes unstable very quickly.\n",
    "# The neural network thus must be trained pretty well to avoid these instabilities.\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [t_wrongp, x_wrongp, \"1d w/ wrong linear\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the wrong feature gave us a very unstable model with advection that does not work.\n",
    "\n",
    "In the following, we will try to learn the advection from a neural network.  This result shows that we need to do something reasonable to have a stable system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the 3-layer non-local neural network\n",
    "\n",
    "Now we can forget about neading to know the right form of the advection term.\n",
    "We are instead just going to throw the information from the advection scheme to the non-local neural network and let it learn the advection for itself.\n",
    "\n",
    "\n",
    "\n",
    "These follow the templates from the exercise led by Janni in week 4.\n",
    "\n",
    "\n",
    "\n",
    "_I'm quite new to neural networks, so please let me know if you see any obvious mistakes in my approach!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm going to start by scaling the data so that it is approximately order 1.\n",
    "\n",
    "It looks like we can scaling $X$ and the advection with the forcing and forcing squared, respectively (we will come back to this assumption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X, Adv, \"k.\")\n",
    "plt.xlabel(\"$X_k$\")\n",
    "plt.ylabel(\"$Adv_k$\")\n",
    "\n",
    "# for F=10\n",
    "X_F = X / Forcing\n",
    "Adv_F = Adv / Forcing**2\n",
    "\n",
    "print(\"Advection RMS:\", np.sqrt(np.mean(Adv**2)))\n",
    "print(\"X RMS:\", np.sqrt(np.mean(X**2)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_F, Adv_F, \"k.\")\n",
    "plt.xlabel(\"$X_k/F$\")\n",
    "plt.ylabel(\"$Adv_k/F^2$\")\n",
    "\n",
    "print(\"Scaled Advection RMS:\", np.sqrt(np.mean(Adv_F**2)))\n",
    "print(\"Scaled X RMS:\", np.sqrt(np.mean(X_F**2)))\n",
    "\n",
    "# Split into 80% training and 20% testing.\n",
    "\n",
    "L = int(len(X) * 0.8)\n",
    "\n",
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.array(X_F[:L])).double(),\n",
    "    torch.from_numpy(np.array(Adv_F[:L])).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"N training data: \", len(X_F[:L]))\n",
    "\n",
    "\n",
    "print(\"N testing data: \", len(X_F[L:]))\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.array(X_F[L:])).double(),\n",
    "    torch.from_numpy(np.array(Adv_F[L:])).double(),\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "\n",
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "        # self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = FF.relu(self.linear1(x))\n",
    "        x = FF.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    net.train()\n",
    "    test_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "\n",
    "\n",
    "def test_model(net, criterion, trainloader, optimizer, text=\"validation\"):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            trainloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "        test_loss /= len(trainloader)  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l = Net_ANN().double()\n",
    "\n",
    "n_epochs = 10  # Number of epocs could be increased\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.03)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how network does for the tendencies\n",
    "\n",
    "preds22 = nn_3l(torch.from_numpy(np.array(X_F)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_F[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 200)\n",
    "plt.ylabel(\"scaled tendency\")\n",
    "plt.xlabel(\"time step\")\n",
    "\n",
    "plt.figure(figsize=(5, 4.5))\n",
    "plt.plot(Adv_F[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.grid(True)\n",
    "plt.plot([-1, 1], [-1, 1], \"y-\")\n",
    "pass\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = nn_3l(torch.from_numpy(np.array(Xt / Forcing)).double()).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"Right\")\n",
    "plt.plot(nnAdv * Forcing**2, \"r-\", label=\"neural network\")\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(\"Advection tendency\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a neural network parameterization in rhs of equation for tendency\n",
    "# The advection will be set to False\n",
    "class GCM_network:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X / self.F).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X / self.F, 0)).double()\n",
    "        return L96_eq1_xdot(\n",
    "            X,\n",
    "            self.F + self.F**2 * np.squeeze(self.network(X_torch).data.numpy()),\n",
    "            advect=False,\n",
    "        )  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test with the parameterization\n",
    "# It takes some time, but usually the network goes unstable eventually\n",
    "\n",
    "# F=10 model\n",
    "gcm_nn = GCM_network(Forcing, nn_3l)\n",
    "xnn, tnn = gcm_nn(init_cond, dt, int(100 / (dt)), nn_3l)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn, xnn, \"1d w/ neural network\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to train the data with slightly higher forcing (will give larger range of advection tendencies to learn)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X, Adv, \"k.\")\n",
    "plt.xlabel(\"$X_k$\")\n",
    "plt.ylabel(\"$Adv_k$\")\n",
    "\n",
    "# for F=20\n",
    "X_F = X_x10 / Forcing_x10\n",
    "Adv_F = Adv_x10 / Forcing_x10**2\n",
    "\n",
    "print(\"Advection RMS:\", np.sqrt(np.mean(Adv**2)))\n",
    "print(\"X RMS:\", np.sqrt(np.mean(X**2)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_F, Adv_F, \"k.\")\n",
    "plt.xlabel(\"$X_k/F$\")\n",
    "plt.ylabel(\"$Adv_k/F^2$\")\n",
    "\n",
    "print(\"Scaled Advection RMS:\", np.sqrt(np.mean(Adv_F**2)))\n",
    "print(\"Scaled X RMS:\", np.sqrt(np.mean(X_F**2)))\n",
    "\n",
    "# Split into 80% training and 20% testing.\n",
    "\n",
    "L = int(len(X) * 0.8)\n",
    "\n",
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.array(X_F[:L])).double(),\n",
    "    torch.from_numpy(np.array(Adv_F[:L])).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"N training data: \", len(X_F[:L]))\n",
    "\n",
    "\n",
    "print(\"N testing data: \", len(X_F[L:]))\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.array(X_F[L:])).double(),\n",
    "    torch.from_numpy(np.array(Adv_F[L:])).double(),\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l_x10 = Net_ANN().double()\n",
    "\n",
    "n_epochs = 10  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_x10.parameters(), lr=0.03)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l_x10, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l_x10, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l_x10, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Look how network does for the tendencies\n",
    "\n",
    "preds22 = nn_3l_x10(torch.from_numpy(np.array(X_F)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_F[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 200)\n",
    "plt.ylabel(\"scaled tendency\")\n",
    "plt.xlabel(\"time step\")\n",
    "\n",
    "plt.figure(figsize=(5, 4.5))\n",
    "plt.plot(Adv_F[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.grid(True)\n",
    "plt.plot([-1, 1], [-1, 1], \"y-\")\n",
    "pass\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = nn_3l_x10(torch.from_numpy(np.array(Xt / Forcing)).double()).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"Right\")\n",
    "plt.plot(nnAdv * Forcing_x10**2, \"r-\", label=\"neural network\")\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(\"Advection tendency\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test with the parameterization built from F=20\n",
    "# Usually the neural network resulting is much more stable, since it is trained for a wider range of conditions.\n",
    "\n",
    "# F=10 model\n",
    "gcm_nn = GCM_network(Forcing, nn_3l_x10)\n",
    "xnn, tnn = gcm_nn(init_cond, dt, int(100 / (dt)), nn_3l_x10)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn, xnn, \"1d w/ neural network\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F=20 model fails still\n",
    "gcm_nn_x10 = GCM_network(Forcing_x10, nn_3l_x10)\n",
    "xnn_x10, tnn_x10 = gcm_nn_x10(init_cond, dt, int(100 / (dt)), nn_3l_x10)\n",
    "\n",
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tnn_x10, xnn_x10, \"1d w/ neural network\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we experiment with adding conservation of \"momentum\"\n",
    "\n",
    "- It turns out the L96 advection does not conserve momentum, but this exercise shows that we can build a parameterization that does by adding it to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss2(inpt, output, target):\n",
    "    # In which we add conservation of \"momentum\" to our loss function\n",
    "    loss = torch.mean(2 * (output - target) ** 2) + torch.mean(\n",
    "        torch.sum(output, axis=1) ** 2\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New training routines that use the new loss function\n",
    "\n",
    "\n",
    "def train_model2(net, criterion, trainloader, optimizer):\n",
    "    net.train()\n",
    "    test_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "        loss = criterion(b_x, prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "\n",
    "\n",
    "def test_model2(net, criterion, trainloader, optimizer, text=\"validation\"):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            trainloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(b_x, prediction, b_y)  # Calculating loss\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "        test_loss /= len(trainloader)  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l_loss2 = Net_ANN().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_loss2.parameters(), lr=0.02)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model2(nn_3l_loss2, my_loss2, loader, optimizer)\n",
    "    train_loss.append(test_model2(nn_3l_loss2, my_loss2, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model2(nn_3l_loss2, my_loss2, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network now conserves momentum\n",
    "\n",
    "preds22o = nn_3l(torch.from_numpy(np.array(X_F)).double())\n",
    "preds22 = nn_3l_loss2(torch.from_numpy(np.array(X_F)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_F[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Adv_F[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.sum(preds22o.detach().numpy(), axis=1), \"r.\", label=\"original loss\")\n",
    "plt.plot(np.sum(preds22.detach().numpy(), axis=1), \"k.\", label=\"new loss\")\n",
    "plt.ylabel(\"Prediction momentum tendency\")\n",
    "plt.legend()\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = nn_3l_loss2(torch.from_numpy(np.array(Xt / Forcing)).double()).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"actual\")\n",
    "plt.plot(nnAdv * Forcing**2, \"r-\", label=\"predicted\")\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't help with stabilization...actually seems to hurt\n",
    "\n",
    "gcm_nn2 = GCM_network(Forcing, nn_3l_loss2)\n",
    "xnn2, tnn2 = gcm_nn2(init_cond, dt, int(T / (dt)), nn_3l_loss2)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn2, xnn2, \"1d w/ NN conserving momentum\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about with some regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l_loss3 = Net_ANN().double()\n",
    "\n",
    "n_epochs = 15  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_loss3.parameters(), lr=0.03, weight_decay=0.0015)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l_loss3, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l_loss3, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l_loss3, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Look how network does for the tendencies\n",
    "\n",
    "preds22 = nn_3l_loss3(torch.from_numpy(np.array(X_F)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_F[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 200)\n",
    "plt.ylabel(\"scaled tendency\")\n",
    "plt.xlabel(\"time step\")\n",
    "\n",
    "plt.figure(figsize=(5, 4.5))\n",
    "plt.plot(Adv_F[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.grid(True)\n",
    "plt.plot([-1, 1], [-1, 1], \"y-\")\n",
    "pass\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = (\n",
    "    nn_3l_loss3(torch.from_numpy(np.array(Xt / Forcing_x10)).double()).detach().numpy()\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"Right\")\n",
    "plt.plot(nnAdv * Forcing_x10**2, \"r-\", label=\"neural network\")\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(\"Advection tendency\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decay regularization can help with stability, but when it does it sometimes ruins the\n",
    "# model representation of the actual 'physics'\n",
    "\n",
    "gcm_nn3 = GCM_network(Forcing, nn_3l_loss3)\n",
    "xnn3, tnn3 = gcm_nn3(init_cond, dt, int(T / (dt)), nn_3l_loss3)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn3, xnn3, \"1d w/ NN momentum reg.\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it actually does an okay job of producing the F=20 simulation\n",
    "\n",
    "gcm_nn3_x10 = GCM_network(Forcing_x10, nn_3l_loss3)\n",
    "xnn3_x10, tnn3_x10 = gcm_nn3_x10(init_cond, dt, int(T / (dt)), nn_3l_loss3)\n",
    "\n",
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tnn3_x10, xnn3_x10, \"1d w/ NN momentum reg.\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different scaling term\n",
    "\n",
    "Could scaling with the Forcing be the issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the advection tendencies, splitting into 80% training and 20% testing.\n",
    "\n",
    "L = int(len(X) * 0.8)\n",
    "print(L)\n",
    "\n",
    "ScX = np.sqrt(np.mean(X**2))\n",
    "X_S = X_x10 / ScX\n",
    "ScA = np.sqrt(np.mean(Adv**2))\n",
    "Adv_S = Adv_x10 / ScA\n",
    "\n",
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.array(X_S[:L])).double(),\n",
    "    torch.from_numpy(np.array(Adv_S[:L])).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.array(X_S[L:])).double(),\n",
    "    torch.from_numpy(np.array(Adv_S[L:])).double(),\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l_loss4 = Net_ANN().double()\n",
    "\n",
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_loss4.parameters(), lr=0.03, weight_decay=0.001)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l_loss4, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l_loss4, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l_loss4, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Look how network does for the tendencies\n",
    "\n",
    "preds22 = nn_3l_loss4(torch.from_numpy(np.array(X_S)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_S[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 200)\n",
    "plt.ylabel(\"scaled tendency\")\n",
    "plt.xlabel(\"time step\")\n",
    "\n",
    "plt.figure(figsize=(5, 4.5))\n",
    "plt.plot(Adv_S[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.grid(True)\n",
    "plt.plot([-1, 1], [-1, 1], \"y-\")\n",
    "pass\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = nn_3l_loss4(torch.from_numpy(np.array(Xt / ScX)).double()).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"Right\")\n",
    "plt.plot(nnAdv * ScA, \"r-\", label=\"neural network\")\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(\"Advection tendency\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a neural network parameterization in rhs of equation for tendency\n",
    "class GCM_network_S:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X / ScX).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X / ScX, 0)).double()\n",
    "        return L96_eq1_xdot(\n",
    "            X,\n",
    "            self.F + ScA * np.squeeze(self.network(X_torch).data.numpy()),\n",
    "            advect=False,\n",
    "        )  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually the network this produces works okay w/ F=10\n",
    "\n",
    "gcm_nn4 = GCM_network_S(Forcing, nn_3l_loss4)\n",
    "xnn4, tnn4 = gcm_nn4(init_cond, dt, int(T / dt), nn_3l_loss4)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn4, xnn4, \"1d w/ rescaled NN mom.\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might work well for F=20\n",
    "\n",
    "gcm_nn4_x10 = GCM_network_S(Forcing_x10, nn_3l_loss4)\n",
    "xnn4_x10, tnn4_x10 = gcm_nn4_x10(init_cond, dt, int(T / dt), nn_3l_loss4)\n",
    "\n",
    "\n",
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tnn4_x10, xnn4_x10, \"1d w/ rescaled NN mom.\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual conservation law should be for \"energy\"\n",
    "\n",
    "Does this stabilize the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss3(inpt, output, target):\n",
    "    # In which we replace conservation of \"momentum\" with conservation of \"energy\"\n",
    "    loss = torch.mean(2 * (output - target) ** 2) + WT * torch.mean(\n",
    "        torch.sum(inpt * output, axis=1) ** 2\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WT = 1\n",
    "\n",
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l_loss5 = Net_ANN().double()\n",
    "\n",
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_loss5.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model2(nn_3l_loss5, my_loss3, loader, optimizer)\n",
    "    train_loss.append(test_model2(nn_3l_loss5, my_loss3, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model2(nn_3l_loss5, my_loss3, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds22o = nn_3l_loss4(torch.from_numpy(np.array(X_S)).double())\n",
    "preds22 = nn_3l_loss5(torch.from_numpy(np.array(X_S)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_S[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Adv_S[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.sum(X * preds22o.detach().numpy() * ScA, axis=1), \"r.\")\n",
    "plt.plot(np.sum(X * preds22.detach().numpy() * ScA, axis=1), \"k.\")\n",
    "plt.ylabel(\"Prediction energy\")\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = nn_3l_loss5(torch.from_numpy(np.array(Xt / Forcing)).double()).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"actual\")\n",
    "plt.plot(nnAdv * Forcing**2, \"r-\", label=\"predicted\")\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_nn5 = GCM_network_S(Forcing, nn_3l_loss5)\n",
    "xnn5, tnn5 = gcm_nn5(init_cond, dt, int(T / dt), nn_3l_loss5)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn5, xnn5, \"1d w/ rescaled NN enrgy.\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_nn5_x10 = GCM_network_S(Forcing_x10, nn_3l_loss5)\n",
    "xnn5_x10, tnn5_x10 = gcm_nn5_x10(init_cond, dt, int(T / dt), nn_3l_loss5)\n",
    "\n",
    "\n",
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tnn5_x10, xnn5_x10, \"1d w/ rescaled NN enrgy.\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a higher weight?\n",
    "\n",
    "WT = 50\n",
    "\n",
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l_loss6 = Net_ANN().double()\n",
    "\n",
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_loss6.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model2(nn_3l_loss6, my_loss3, loader, optimizer)\n",
    "    train_loss.append(test_model2(nn_3l_loss6, my_loss3, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model2(nn_3l_loss6, my_loss3, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds22o = nn_3l_loss5(torch.from_numpy(np.array(X_S)).double())\n",
    "preds22 = nn_3l_loss6(torch.from_numpy(np.array(X_S)).double())\n",
    "plt.figure()\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 2], label=\"NN Predicted values\")\n",
    "plt.plot(Adv_S[:1000, 2], label=\"True values\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Adv_S[:1000, 2], preds22.detach().numpy()[0:1000, 2], \"k.\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.sum(preds22o.detach().numpy(), axis=1) * ScA, \"r.\")\n",
    "plt.plot(np.sum(preds22.detach().numpy(), axis=1) * ScA, \"k.\")\n",
    "plt.ylabel(\"Prediction momentum\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.sum(X * preds22o.detach().numpy(), axis=1) * ScA, \"r.\")\n",
    "plt.plot(np.sum(X * preds22.detach().numpy(), axis=1) * ScA, \"k.\")\n",
    "plt.ylabel(\"Prediction energy\")\n",
    "\n",
    "Xt = init_cond\n",
    "Advr = -np.roll(Xt, 1) * (np.roll(Xt, 2) - np.roll(Xt, -1))\n",
    "nnAdv = nn_3l_loss6(torch.from_numpy(np.array(Xt / Forcing)).double()).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Advr, \"k-\", label=\"actual\")\n",
    "plt.plot(nnAdv * Forcing**2, \"r-\", label=\"predicted\")\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_nn6 = GCM_network_S(Forcing, nn_3l_loss6)\n",
    "xnn6, tnn6 = gcm_nn6(init_cond, dt, int(T / dt), nn_3l_loss6)\n",
    "\n",
    "gcm_nn6_x10 = GCM_network_S(Forcing_x10, nn_3l_loss6)\n",
    "xnn6_x10, tnn6_x10 = gcm_nn6_x10(init_cond, dt, int(T / dt), nn_3l_loss6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnn6, xnn6, \"1d w/ rescaled NN enrgy 20\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tnn6_x10, xnn6_x10, \"1d w/ rescaled NN enrgy 20\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN inside of time-stepping algorithm\n",
    "\n",
    "One issue may be that the network is applied as a forward Euler step.\n",
    "Let's try moving the network inside the RHS that is passed to the RK4 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def L96_eq1_xdot_NN(X, F, NN, advect=True):\n",
    "    \"\"\"\n",
    "    Calculate the time rate of change for the X variables for the Lorenz '96, equation 1:\n",
    "        d/dt X[k] = -X[k-2] X[k-1] + X[k-1] X[k+1] - X[k] + F\n",
    "\n",
    "    Args:\n",
    "        X : Values of X variables at the current time step\n",
    "        F : Forcing term\n",
    "    Returns:\n",
    "        dXdt : Array of X time tendencies\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(X)\n",
    "    Xdot = np.zeros(K)\n",
    "    if NN.linear1.in_features == 1:\n",
    "        X_torch = torch.from_numpy(X / ScX).double()\n",
    "        X_torch = torch.unsqueeze(X_torch, 1)\n",
    "    else:\n",
    "        X_torch = torch.from_numpy(np.expand_dims(X / ScX, 0)).double()\n",
    "\n",
    "    if advect:\n",
    "        Xdot = np.roll(X, 1) * (np.roll(X, -1) - np.roll(X, 2)) - X + F\n",
    "    else:\n",
    "        Xdot = -X + F + ScA * np.squeeze(NN(X_torch).data.numpy())\n",
    "    #     for k in range(K):\n",
    "    #         Xdot[k] = ( X[(k+1)%K] - X[k-2] ) * X[k-1] - X[k] + F\n",
    "    return Xdot\n",
    "\n",
    "\n",
    "# - a GCM class including a neural network parameterization in rhs of equation for tendency\n",
    "class GCM_network_tsNN:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot_NN(\n",
    "            X, self.F, self.network, advect=False\n",
    "        )  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test with the parameterization\n",
    "\n",
    "gcm_nnRK = GCM_network_tsNN(Forcing, nn_3l_x10, time_stepping=RK4)\n",
    "xnnRK, tnnRK = gcm_nnRK(init_cond, dt, int(100 / (dt)), nn_3l)\n",
    "\n",
    "CompExps(\n",
    "    [t2d, x2d, \"2d\"],\n",
    "    [\n",
    "        [tnnRK, xnnRK, \"1d w/ RK4 neural network\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test with the parameterization\n",
    "\n",
    "gcm_nnRK_x10 = GCM_network_tsNN(Forcing_x10, nn_3l_x10, time_stepping=RK4)\n",
    "xnnRK_x10, tnnRK_x10 = gcm_nnRK_x10(init_cond, dt, int(100 / (dt)), nn_3l)\n",
    "\n",
    "CompExps(\n",
    "    [t2d_x10, x2d_x10, \"2d\"],\n",
    "    [\n",
    "        [tnnRK_x10, xnnRK_x10, \"1d w/ RK4 neural network\"],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- Conservation properties can be added to the loss function, but may not improve stability.\n",
    "  - Conservation can unintentially over-regulate a network.\n",
    "- Training a network for a wider parameter space than the model sees can help with stability.\n",
    "  - Training with F=20 helps F=10 stay stable\n",
    "  - Training for too broad of a parameter space may limit model ability to capture complex behavior (not shown, with F=100 tuning)\n",
    "- Careful scaling is needed to help extrapolate results across parameter space. \n",
    "  - It was wrong to scale with forcing, scaling from the mean helps.  \n",
    "  \n",
    "  \n",
    "  \n",
    "- We could also consider more stability approaches, for example:\n",
    "  - How you build the parameterization matters.  Building a parameterization for a flux instead of a flux tendency can help avoid non-conservation (not as applicable to our problem here, but seen in boundary layer parameterizations).\n",
    "  - Coupled online learning can help tune networks that can learn evolving parameter spaces (see Rasp 2020 and their notebooks).\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
