{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating saliency maps for neural networks trained on L96\n",
    "\n",
    "Saliency maps are popular visualization techniques for gaining insights on why neural networks made a particular decision on the given input data. They are usually rendered as a heatmap, where hotness corresponds to regions that have a big impact on the modelâ€™s final decision. They are helpful, for example, when you are frustrated by your model incorrectly classifying a certain datapoint, because you can look at the input features that led to that decision {cite}`Simonyan2013`, {cite}`Baehrens2010`, {cite}`Adebayo2018`.\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we will explore the different ways one could render saliency maps for the L96 data and visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate saliency maps using input gradients\n",
    "\n",
    "Since neural networks are differentiable, the simplest way of generating saliency maps (i.e. a quantification of the sensitivity of the output to the input) is to just take its first derivative with respect to its inputs (or Jacobian, for networks with multiple inputs and outputs).\n",
    "\n",
    "We can do this easily in Pytorch using `torch.autograd.functional.jacobian`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.autograd.functional import jacobian\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from L96_model import (\n",
    "    L96,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    "    EulerFwd,\n",
    "    RK2,\n",
    "    RK4,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix like figure\n",
    "def imshow(x, colorbar_pct=97.5, cmap=\"seismic\", label=None, vlim=None, **kw):\n",
    "    if vlim is None:\n",
    "        vlim = np.percentile(np.abs(x), colorbar_pct)\n",
    "    plt.xlabel(\"Input dimension\", fontsize=14)\n",
    "    plt.ylabel(\"Output dimension\", fontsize=14)\n",
    "    plt.xticks(range(8))\n",
    "    plt.yticks(range(8))\n",
    "    im = plt.imshow(x, vmin=-vlim, vmax=vlim, cmap=cmap, **kw)\n",
    "    cb = plt.colorbar()\n",
    "    if label is not None:\n",
    "        cb.set_label(label, fontsize=14)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate L96 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "Forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"synthetic world\" with K=8 and J=32\n",
    "K = 8\n",
    "J = 32\n",
    "W = L96(K, J, F=Forcing)\n",
    "# Get training data for the neural network.\n",
    "\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path\n",
    "PATH = \"networks/network_3_layers_100_epoches.pth\"\n",
    "# Load\n",
    "weights = torch.load(PATH)\n",
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_ANN()\n",
    "model.load_state_dict(weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Jacobian method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jacobians = np.array(\n",
    "    [\n",
    "        jacobian(\n",
    "            model,\n",
    "            torch.tensor(np.single(X_true[i, :]), requires_grad=True),\n",
    "            create_graph=False,\n",
    "        )\n",
    "        .detach()\n",
    "        .numpy()\n",
    "        for i in range(200)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an array of 8x8 gradients, one for each of 200 input samples. Let's visualize their average value, as well as their standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "fig.suptitle(\n",
    "    \"Mean and standard deviation of NN input gradients across the dataset\",\n",
    "    fontsize=18,\n",
    "    y=1.025,\n",
    ")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Average value\", fontsize=16)\n",
    "imshow(jacobians.mean(0), label=\"Average input derivative\", vlim=0.85)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Standard deviation\", fontsize=16)\n",
    "imshow(jacobians.std(0), label=\"Standard deviation\", vlim=0.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant term in the average gradient is close to -1 along the main diagonal, but there are significant off-diagonal elements, and also significant deviation across samples. It's interesting to compare this to the behavior of a linear regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_true, xy_true)\n",
    "\n",
    "plt.title(\"Comparing to linear model\", fontsize=16)\n",
    "imshow(lr.coef_, label=\"Linear regression weight\", vlim=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the weights of a linear regression model generally match the average input gradients of the neural network, especially down the main diagonal. This makes some sense given that, at each point, input gradients represent the best _local_ linear model that approximates the nonlinear neural network.\n",
    "\n",
    "Although computing full Jacobians works fine for a small example, it can become expensive for large networks and input/output dimensions, so we can also approximate it using finite differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Jacobian method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epsilon = 1e-2\n",
    "approx_jacobians = []\n",
    "\n",
    "for case in range(200):\n",
    "    inputs = X_true[case, :].copy()\n",
    "    inputs = torch.tensor(np.single(inputs), requires_grad=False)\n",
    "    pred = model(inputs)\n",
    "    Js = np.zeros((len(inputs), len(pred)))\n",
    "    for j in range(len(inputs)):\n",
    "        perturb = np.zeros_like(inputs)\n",
    "        perturb[j] = epsilon\n",
    "        inputs_perturbed = (inputs + perturb).clone().detach().requires_grad_(False)\n",
    "        Js[j, :] = (\n",
    "            model(inputs_perturbed).detach().numpy() - pred.detach().numpy()\n",
    "        ) / epsilon\n",
    "    approx_jacobians.append(Js.T)\n",
    "\n",
    "approx_jacobians = np.array(approx_jacobians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Technically this is slightly slower than the previous example, but it can be more performant for large networks.)\n",
    "\n",
    "Let's see how that looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Interpreting the network with finite differences\", fontsize=16)\n",
    "imshow(approx_jacobians.mean(0), label=\"Average finite difference\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As expected, it's fairly similar to the full Jacobian method.\n",
    "\n",
    "## Generate saliency maps layerwise relevance propagation (LRP)\n",
    "\n",
    "Another proposed method for generating saliency maps is {cite}`Bach2015`. With a baseline of 0, this method is akin to multiplying the input gradients by the input itself (per {cite}`Ancona2018`), but it has become popular in the climate community and does support alternative baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in weights.keys():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0  # filtering small values\n",
    "gamma = 0.0  # give more weights to positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the weight and bias of the NN\n",
    "def get_weight(weightsname):\n",
    "    Ws = []\n",
    "    Bs = []\n",
    "    for i, name in enumerate(weights.keys()):\n",
    "        if i % 2 == 0:\n",
    "            Ws.append(np.array(weights[name]))\n",
    "        else:\n",
    "            Bs.append(np.array(weights[name]))\n",
    "    return Ws, Bs  # weights and biases\n",
    "\n",
    "\n",
    "# forward pass to calculate the output of each layer\n",
    "def forward_pass(data, Ws, Bs):\n",
    "    L = len(Ws)\n",
    "    forward = [data] + [None] * L\n",
    "\n",
    "    for l in range(L - 1):\n",
    "        forward[l + 1] = np.maximum(0, Ws[l].dot(forward[l])) + Bs[l]  # ativation ReLu\n",
    "\n",
    "    ## for last layer that does not have activation function\n",
    "    forward[L] = Ws[L - 1].dot(forward[L - 1]) + Bs[L - 1]  # linear last layer\n",
    "    return forward\n",
    "\n",
    "\n",
    "def rho(w, l):\n",
    "    w_intermediate = w + [0.0, 0.0, 0.0, 0.0, 0.0][l] * np.maximum(0, w)\n",
    "    return w_intermediate + gamma * np.maximum(0, w_intermediate)\n",
    "\n",
    "\n",
    "def incr(z, l):\n",
    "    return z + [0.0, 0.0, 0.0, 0.0, 0.0][l] * (z**2).mean() ** 0.5 + 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backward pass to compute the LRP of each layer. Same rule applied to the first layer (input layer)\n",
    "def onelayer_LRP(W, B, forward, nz, zz):\n",
    "    mask = np.zeros((nz))\n",
    "    mask[zz] = 1\n",
    "    L = len(W)\n",
    "    R = [None] * L + [forward[L] * mask]  # start from last layer Relevance\n",
    "\n",
    "    for l in range(0, L)[::-1]:\n",
    "        w = rho(W[l], l)\n",
    "        b = rho(B[l], l)\n",
    "        z = incr(w.dot(forward[l]) + b + epsilon, l)  # step 1 - forward pass\n",
    "        s = np.array(R[l + 1]) / np.array(z)  # step 2 - element-wise division\n",
    "        c = w.T.dot(s)  # step 3 - backward pass\n",
    "        R[l] = forward[l] * c  # step 4 - element-wise product\n",
    "    return R\n",
    "\n",
    "\n",
    "def LRP_alllayer(data, weights):\n",
    "    \"\"\"inputs:\n",
    "        data: for single sample, with the right asix, the shape is (nz,naxis)\n",
    "        weights: dictionary of weights and biases\n",
    "    output:\n",
    "        LRP, shape: (nx,L+1) that each of the column consist of L+1 array\n",
    "        Relevance of fisrt layer's pixels\"\"\"\n",
    "    nx = data.shape[0]\n",
    "    ## step 1: get the wieghts\n",
    "    Ws, Bs = get_weight(weights)\n",
    "\n",
    "    ## step 2: call the forward pass to get the intermediate layers output\n",
    "    inter_layer = forward_pass(data, Ws, Bs)\n",
    "\n",
    "    ## loop over all z and get the LRP of each layer\n",
    "    R_all = [None] * nx\n",
    "    relevance = np.zeros((nx, nx))\n",
    "    for xx in range(nx):\n",
    "        R_all[xx] = onelayer_LRP(Ws, Bs, inter_layer, nx, xx)\n",
    "        relevance[xx, :] = R_all[xx][0]\n",
    "\n",
    "    return np.array(R_all, dtype=object), relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "R_many = []\n",
    "for case in range(200):\n",
    "    inputs = np.copy(X_true[case, :])\n",
    "    _, Rs = LRP_alllayer(inputs, weights)\n",
    "    R_many.append(Rs)\n",
    "LRP = np.stack(R_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Interpreting the network with\\nlayerwise relevance propagation\", fontsize=16)\n",
    "imshow(LRP.mean(0), label=\"Average LRP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LRP outputs something qualitatively different from the gradient-based methods, and in fact each element should be interpreted more as an attribution score (i.e. the actual contribution of an input to the output) than as a sensitivity score (i.e. how much an output changes with an input). Below, we'll see that this approximately reduces to multiplying the gradient by the input.\n",
    "\n",
    "## Comparing all of the methods\n",
    "\n",
    "Let's look at the output of these methods side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = [\n",
    "    (jacobians, \"Jacobian (exact)\"),\n",
    "    (approx_jacobians, \"Jacobian (finite diff)\"),\n",
    "    (LRP, \"LRP\"),\n",
    "    (jacobians * X_true[:200][:, np.newaxis, :], \"Jacobian * input\"),\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.suptitle(\n",
    "    \"Comparing average outputs of saliency methods\", fontsize=20, y=1.0, va=\"bottom\"\n",
    ")\n",
    "\n",
    "for i, (saliency, label) in enumerate(comparisons):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(label, fontsize=16)\n",
    "    imshow(saliency.mean(0), label=label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "As expected, LRP and Jacobian * input produce very similar outputs. For more information on the intricacies of saliency maps, see {cite}`Ancona2018` and {cite}`Bach2015`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
