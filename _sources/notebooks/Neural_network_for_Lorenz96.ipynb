{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Using Neural Networks for L96 Parameterization\n",
    "\n",
    "In this notebook, we'll extend upon the concepts learned in the [previous](https://m2lines.github.io/L96_demo/notebooks/Universal_approximation.html) notebook by using deep neural networks for Lorenz 96 parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from L96_model import L96, RK4, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Setting up the Dataset, Network, and the Training Code\n",
    "\n",
    "First, we setup all the necessary code that's required to build the dataset, create our linear network and train the network on the dataset.\n",
    "\n",
    "```{note}\n",
    "The dataset, the linear network, and the training and evaluation functions that we use in this notebook are same as the one defined in [Introduction to Neural Networks](https://m2lines.github.io/L96_demo/notebooks/Universal_approximation.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Generating the Ground Truth\n",
    "# ---------------------------\n",
    "\n",
    "time_steps = 20000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "W = L96(8, 32, F=forcing)\n",
    "\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)\n",
    "X_true, xy_true = X_true.astype(np.float32), xy_true.astype(np.float32)\n",
    "\n",
    "\n",
    "# Splitting the into Training and Test Dataset\n",
    "# --------------------------------------------\n",
    "\n",
    "val_size = 4000\n",
    "\n",
    "# Training Data\n",
    "X_true_train = X_true[:-val_size, :]\n",
    "subgrid_tend_train = xy_true[:-val_size, :]\n",
    "\n",
    "# Test Data\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_tend_test = xy_true[-val_size:, :]\n",
    "\n",
    "\n",
    "# Building the Dataset and the Dataloaders\n",
    "# ----------------------------------------\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# Training Data\n",
    "local_data_train = TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_train, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_train, -1)),\n",
    ")\n",
    "local_loader_train = DataLoader(\n",
    "    dataset=local_data_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# Test Data\n",
    "local_data_test = TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_test, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_test, -1)),\n",
    ")\n",
    "local_loader_test = DataLoader(\n",
    "    dataset=local_data_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Define Functions to Train and Evaluate Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)\n",
    "\n",
    "\n",
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Create and Train a Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the Network Class\n",
    "# ------------------------\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)  # A single input and a single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method is automatically executed when\n",
    "        # we call a object of this class\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the network\n",
    "# ----------------------\n",
    "linear_network = LinearRegression()\n",
    "\n",
    "\n",
    "# Train the linear model\n",
    "# ----------------------\n",
    "n_epochs = 3\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(linear_network.parameters(), lr=0.03)\n",
    "_, _ = fit_model(\n",
    "    linear_network,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    local_loader_train,\n",
    "    local_loader_test,\n",
    "    n_epochs,\n",
    ")\n",
    "\n",
    "\n",
    "# Get the weights of the model\n",
    "# ----------------------------\n",
    "linear_network_weights = np.array(\n",
    "    [\n",
    "        linear_network.linear1.weight.data.numpy()[0][0],\n",
    "        linear_network.linear1.bias.data.numpy()[0],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Adding Simple Linear Parameterization to GCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In all the GCMs, we set time stepping using the fourth order Runge-Kutta method.\n",
    "\n",
    "To recap, {cite}`Lorenz1995` describes a two-time scale dynamical system using two equations which are:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\end{gather*}\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{note}\n",
    "All the GCM networks used in this notebook have been introduced earlier in notebooks [Key aspects of GCMs parameterizations](https://m2lines.github.io/L96_demo/notebooks/gcm-parameterization-problem.html) and [Tuning GCM Parameterizations](https://m2lines.github.io/L96_demo/notebooks/estimating-gcm-parameters.html). To see the definition of those networks, expand the cells in the respective GCM sections below.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 10\n",
    "\n",
    "# Full L96 model\n",
    "X_full, _, _ = W.run(dt, T_test)\n",
    "X_full = X_full.astype(np.float32)\n",
    "\n",
    "init_conditions = X_true[-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### GCM *Without* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class GCM_without_parameterization:\n",
    "    \"\"\"GCM without parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_no_param = GCM_without_parameterization(forcing)\n",
    "X_no_param, t = gcm_no_param(init_conditions, dt, int(T_test / dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### GCM Linear Parameterization in RHS of Equation for Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class GCM_linear_parameterization:\n",
    "    \"\"\"GCM with linear parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        parameterization: Parameterization function\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, parameterization, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "gcm = GCM_linear_parameterization(forcing, naive_parameterization)\n",
    "X_param, t = gcm(init_conditions, dt, int(T / dt), param=-linear_network_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### GCM *With* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class GCM_network:\n",
    "    \"\"\"GCM with neural network parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        network: Neural network\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, network, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X)\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0))\n",
    "\n",
    "        # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_net = GCM_network(forcing, linear_network)\n",
    "Xnn_1layer, t = gcm_net(init_conditions, dt, int(T_test / dt), linear_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Comparing Results\n",
    "\n",
    "Comparing the predictions of GCM with different parameterizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 200\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_full[:time_i, 4], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, 4], \".\", label=\"NN 1 layer\")\n",
    "plt.plot(t[:time_i], X_no_param[:time_i, 4], label=\"No parameterization\")\n",
    "plt.plot(t[:time_i], X_param[:time_i, 4], label=\"linear parameterization\")\n",
    "plt.legend(loc=\"upper left\", fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Using Deeper Networks for Lorenz 96 (with Non-Local Features)\n",
    "\n",
    "Now we'll increase the complexity of our neural network by adding a few more linear layers to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Create Non-Local Training and Test Dataset\n",
    "\n",
    "We first start by generating the dataset which has *8 inputs* and *8 outputs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "# ----------------\n",
    "nlocal_data_train = TensorDataset(\n",
    "    torch.from_numpy(X_true_train),\n",
    "    torch.from_numpy(subgrid_tend_train),\n",
    ")\n",
    "loader_train = DataLoader(\n",
    "    dataset=nlocal_data_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# Test Dataset\n",
    "# ------------\n",
    "nlocal_data_test = TensorDataset(\n",
    "    torch.from_numpy(X_true_test), torch.from_numpy(subgrid_tend_test)\n",
    ")\n",
    "loader_test = DataLoader(dataset=nlocal_data_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Creating a 3 layer Neural Network with ReLU Activation\n",
    "\n",
    "We now build a 3 layer neural network consisting of two hidden layers and an output layer. This time we use an activation function called `ReLU` (a very common choice) after every hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{admonition} Need for Activation Functions\n",
    "\n",
    "**If layers of a neural network contain only fully-connected layers (matrix multiplications), everything would be linear.**\n",
    "\n",
    "For example, if we have an input $x$ along with 2 layers of weight matrices $A$ and $B$ then the neural network would compute the output as $A(Bx)$, which is linear (in $x$). Thus, in order to introduce some non-linearity we use activation functions.\n",
    "\n",
    "Now the same neural network as above with an activation function $\\phi$ would compute the output as $A(\\phi(Bx))$.\n",
    "```\n",
    "\n",
    "`````{admonition} ReLU Activation Function\n",
    "\n",
    "```{figure} figs/relu_activation_function.png\n",
    ":name: relu-activation\n",
    "```\n",
    "\n",
    "The `ReLU` activation function is just a $max(0,X)$ function (The image is referenced from [this blog](https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/)). Even a function as simple as this enables a typical NN to be a nonlinear function of the inputs!\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Training the 3-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the model\n",
    "nn_3l = NetANN()\n",
    "\n",
    "# Setup the optimizer and loss\n",
    "n_epochs = 50\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.003)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l, criterion, optimizer, loader_train, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Training and Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Comparing Predictions with the Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_3l(torch.from_numpy(X_true_test[:, :]))\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(predictions.detach().numpy()[0:1000, 1], label=\"NN Predicted values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True values\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Adding Deep Neural Network Parameterization to GCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 5\n",
    "\n",
    "# GCM parameterized by the global 3-layer network\n",
    "gcm_net_3layers = GCM_network(forcing, nn_3l)\n",
    "Xnn_3layer, t = gcm_net_3layers(init_conditions, dt, int(T_test / dt), nn_3l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Comparing Results with Linear Network Parameterized GCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCM parameterized by the linear network\n",
    "gcm_net_1layers = GCM_network(forcing, linear_network)\n",
    "Xnn_1layer, t = gcm_net_1layers(init_conditions, dt, int(T_test / dt), linear_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 240\n",
    "channel = 1\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_full[:time_i, channel], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, channel], \".\", label=\"NN 1 layer local\")\n",
    "plt.plot(t[:time_i], Xnn_3layer[:time_i, channel], \".\", label=\"NN 3 layer global\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Checking over 100 Different Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "err_1l, err_3l = [], []\n",
    "T_test = 1\n",
    "for i in range(100):\n",
    "    init_conditions_i = X_true[i * 10, :]\n",
    "\n",
    "    # GCM parameterized by the global 3-layer network\n",
    "    gcm_net_3layers = GCM_network(forcing, nn_3l)\n",
    "    Xnn_3layer_i, t = gcm_net_3layers(init_conditions_i, dt, int(T_test / dt), nn_3l)\n",
    "\n",
    "    # GCM parameterized by the linear network\n",
    "    gcm_net_1layers = GCM_network(forcing, linear_network)\n",
    "    Xnn_1layer_i, t = gcm_net_1layers(\n",
    "        init_conditions_i, dt, int(T_test / dt), linear_network\n",
    "    )\n",
    "\n",
    "    err_1l.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_1layer_i))\n",
    "    )\n",
    "    err_3l.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_3layer_i))\n",
    "    )\n",
    "\n",
    "print(f\"Sum of errors for 1 layer local: {sum(err_1l):.2f}\")\n",
    "print(f\"Sum of errors for 3 layer global: {sum(err_3l):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Training the Model Further to Improve Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l, criterion, optimizer, loader_train, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Plotting the Training and Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Saving the Network\n",
    "\n",
    "Let's save the weights of the trained network so that we don't have to train it again if we want to use it in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network\n",
    "save_path = \"./networks/network_3_layers_100_epoches.pt\"\n",
    "torch.save(nn_3l.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
