{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d2675f-796a-4466-a76a-c9a354518d1b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa918476-598e-4ec0-9d9c-b2fd20d2e699",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Outline\n",
    "\n",
    "The **Universal Approximation Theorm** states that neural networks can approximate any continuous function. A visual demonstration that neural nets can compute any function can be seen in [this page](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "In this notebook, we give a brief overview of neural networks and how to build them using PyTorch. If you want to go through it in depth, check out these resources:\n",
    "- [Deep Learning With Pytorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a18f4-f3d5-4503-8c0b-e81895ee2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot, integrate_L96_2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79044f9a-ac01-415a-a88a-0385718c8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555922c-f58e-4343-8cf0-1ad027046c6a",
   "metadata": {},
   "source": [
    "### Build the *Real World* to Generate the Ground Truth Dataset\n",
    "\n",
    "We initialise the L96 two time-scale model using $K$ (set to 8) values of $X$ and $J$ (set to 32) values of $Y$ for each $X$. The model is run for 20,000 timesteps to generate the dataset for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0738e8-fd6f-4f0c-befa-aa5db97f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df4af7-7283-4a64-8a3d-cbb7dada4363",
   "metadata": {},
   "source": [
    "### Getting Training Data\n",
    "\n",
    "Using the *real world* model created above we generate the training data (input and output pairs) for the neural network by running the true state and outputting subgrid tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3e6b4-269f-4ac5-82e8-366eb2fb6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The effect of Y on X is `xy_true`\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)\n",
    "\n",
    "# Change the data type to `float32` in order to avoid doing type conversions later on\n",
    "X_true, xy_true = X_true.astype(np.float32), xy_true.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586ecb-43a0-4764-9b6f-c968be40eca9",
   "metadata": {},
   "source": [
    "### Split the Data to obtain the Training and Test (Validation) Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99293a-df6d-4d60-b177-03fc2f8ae361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps for validation\n",
    "val_size = 4000\n",
    "\n",
    "# Training Data\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_tend_train = xy_true[:-val_size, :]\n",
    "\n",
    "# Test Data\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_tend_test = xy_true[-val_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac52993-712b-4aba-8012-7ca83a4d605e",
   "metadata": {},
   "source": [
    "### Create Dataloaders \n",
    "\n",
    "- `Dataset` and `Dataloader` classes provide a very convenient way of iterating over a dataset while training a deep learning model.\n",
    "\n",
    "- We need to iterate over the data because it is very slow and memory-intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e751231-d506-4bee-982d-c7522a4481e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sample in each batch\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615518a-e6d8-4a9d-ad05-b334fd7f8224",
   "metadata": {},
   "source": [
    "Define the X (state), Y (subgrid tendency) pairs for the linear regression local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e3982-1791-418e-bf8d-81475da84a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_train, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_train, -1)),\n",
    ")\n",
    "\n",
    "local_loader = Data.DataLoader(\n",
    "    dataset=local_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ef6f0-9441-49c5-8cea-904c992d75c1",
   "metadata": {},
   "source": [
    "Define the dataloader for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec82d6-91e3-4851-993f-fa28836af5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_test, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_test, -1)),\n",
    ")\n",
    "\n",
    "local_loader_test = Data.DataLoader(\n",
    "    dataset=local_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff24b3-f8c9-4b5e-a36f-82d6b0f177d1",
   "metadata": {},
   "source": [
    "Display a batch of samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527ff9a-01c8-419a-9ca4-c28a38a54b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the data to get one batch\n",
    "data_iterator = iter(local_loader)\n",
    "X_iter, subgrid_tend_iter = next(data_iterator)\n",
    "\n",
    "print(\"X (State):\\n\", X_iter)\n",
    "print(\"\\nY (Subgrid Tendency):\\n\", subgrid_tend_iter)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(X_iter, subgrid_tend_iter, \".\")\n",
    "plt.xlabel(\"State\", fontsize=20)\n",
    "plt.ylabel(\"Subgrid tendency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eff251-d0c0-4628-b617-6189915e9daa",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Neural Network Architectures\n",
    "\n",
    "We will try to understand the fully connected networks with the help of Linear regression (and gradient descent).\n",
    "\n",
    "```{figure} https://miro.medium.com/max/720/1*VHOUViL8dHGfvxCsswPv-Q.png\n",
    ":name: neural-network\n",
    ":width: 600\n",
    "\n",
    "A neural network with 4 hidden layers and an output layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e53bd-fb12-4aad-bcff-be495935ef80",
   "metadata": {},
   "source": [
    "### Building a Linear Regression Network\n",
    "\n",
    "First, we will build a linear regression \"network\" and later see how to generalize the linear regression in order to use fully connected neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1ac73-a213-4d2c-81af-d74b093d2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)  # A single input and a single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method is automatically executed when\n",
    "        # we call a object of this class\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a3d2f-7f95-4906-a3b0-dd2ecf81822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_network = LinearRegression()\n",
    "linear_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f6fd5-0055-4583-8677-ae31ac9953e9",
   "metadata": {},
   "source": [
    "### Test forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b236b7f-dd79-4f88-8e8a-e60bfbc2a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = torch.randn(1, 1)\n",
    "out = linear_network(net_input)\n",
    "print(f\"The output of the random input is: {out.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471c571-5ee4-48ef-bfce-e2349bcdfe5b",
   "metadata": {},
   "source": [
    "### Defining the Loss Function\n",
    "\n",
    "In order to check how well our network is modeling the dataset, we need to define a loss function. For our task, we choose the *Mean Squared Error* metric as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42de43-27de-4896-91ee-caf695f212df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c7361-f717-4a74-b296-075f398d7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input and output pair from the data loader\n",
    "X_tmp = next(iter(local_loader))\n",
    "\n",
    "# Predict the output\n",
    "y_tmp = linear_network(torch.unsqueeze(X_tmp[0], 1))\n",
    "\n",
    "# Calculate the MSE loss\n",
    "loss = criterion(y_tmp, torch.unsqueeze(X_tmp[1], 1))\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041270e-2857-4dce-bcd7-05f61694deab",
   "metadata": {},
   "source": [
    "### Calculating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877e848-2f98-4bb9-b5a2-eb80fdefb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters\n",
    "linear_network.zero_grad()\n",
    "\n",
    "print(\"Gradients before backward:\")\n",
    "print(linear_network.linear1.bias.grad)\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(\"\\nGradients after backward:\")\n",
    "print(linear_network.linear1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98716e66-ed26-48f4-8509-9f25e0ddeebf",
   "metadata": {},
   "source": [
    "### Updating the Weights using an Optimizer\n",
    "\n",
    "Now in order to make the network learn, we need an algorithm that will update its weights depending on the loss function. This is achieved by using an optimizer. The implementation of almost every optimizer that we'll ever need can be found in PyTorch itself. The choice of which optimizer we choose might be very important as it will determine how fast the network will be able to learn.\n",
    "\n",
    "In the example below, we show one of the popular optimizers `SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed28fd9-441e-4e01-9e3c-7571ff7a311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(linear_network.parameters(), lr=0.003, momentum=0.9)\n",
    "print(\"Before backward pass: \\n\", list(linear_network.parameters())[0].data.numpy())\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nAfter backward pass: \\n\", list(linear_network.parameters())[0].data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2fefc-1479-43b2-b8de-24321a0dff60",
   "metadata": {},
   "source": [
    "An optimizer usually consists of two major hyperparameters called the **learning rate** and **momentum**. The **learning rate** determines the magnitude with which the weights of the network update thus making it crucial to choose the correct learning rate ($LR$) otherwise the network will either fail to train, or take much longer to converge. To read about **momentum**, check out this [blog post](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d).\n",
    "\n",
    "The  effective value of the gradient $V$ at step $t$ in SGD with momentum ($\\beta$) is determined by\n",
    "\n",
    "\\begin{equation}\n",
    "V_t = \\beta V_{t-1} + (1-\\beta) \\nabla_w L(W,X,y)\n",
    "\\end{equation}\n",
    "\n",
    "and the updates to the weights will be\n",
    "\n",
    "\\begin{equation}\n",
    "w^{new} = w^{old} - LR * V_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a2478-e1f7-45db-b9e0-a9a511a4a912",
   "metadata": {},
   "source": [
    "#### Adam Optimizer\n",
    "\n",
    "Another popular optimizer that is used in many neural networks is the Adam optimizer. It is an adaptive learning rate method that computes individual learning rates for different parameters. For further reading, check out this [post](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) about Adam, and this [post](https://www.ruder.io/optimizing-gradient-descent/) about other optimizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28f566-bb81-4d2d-9acc-b24ae80712c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combining it all Together: Training the Whole Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66f8a8-8c11-4897-b71e-ac06d03271ac",
   "metadata": {},
   "source": [
    "### Define the Training and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800f28f-69c5-40eb-9b0c-cb27059f2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963a83c-c7ba-42a6-9642-002e5d254c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8b24a-3107-48b4-8b52-d26776fa95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c15e2-c4ea-4de7-9fe6-2f5cbc656f03",
   "metadata": {},
   "source": [
    "### Set Hyperparameters\n",
    "\n",
    "Epochs refer to the number of times we iterate over the entire training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447887a6-4c7e-4146-b7c9-0962594ad997",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "optimizer = optim.Adam(linear_network.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052dcbd-a72d-40d8-a9ba-4657630e48ae",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39cc83-2cb4-4e54-95cf-e11b6240dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = fit_model(\n",
    "    linear_network, criterion, optimizer, local_loader, local_loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b387b0-3f08-4871-a170-1fcb029df63d",
   "metadata": {},
   "source": [
    "### Show the Weights of the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc206-7c55-4813-ab65-4259a9290609",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(\n",
    "    [\n",
    "        linear_network.linear1.weight.data.numpy()[0][0],\n",
    "        linear_network.linear1.bias.data.numpy()[0],\n",
    "    ]\n",
    ")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615e08d-6399-49e4-9c7f-0cbd171d084f",
   "metadata": {},
   "source": [
    "### Compare Predictions with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aab879-0337-4b5e-8dc9-cdb9497c9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_network(\n",
    "    torch.unsqueeze(torch.from_numpy(np.reshape(X_true_test[:, 1], -1)), 1)\n",
    ")\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(predictions.detach().numpy()[0:1000], label=\"Predicted Values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True Values\")\n",
    "plt.legend(fontsize=7);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
