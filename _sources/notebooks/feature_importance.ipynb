{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "Feature importance is a technique that allows us to identify the most contributing features in a given dataset by some scoring method. The score can be defined by any model evaluated on the dataset. However, users must be careful while selecting the evaluation critieria and model as the importance of a particular input feature is dependent on these two choices.\n",
    "\n",
    "We score each feature by scrambling its values and fitting the model. The relative change in performance of the model fit on the scrambled feature dataset versus the original feature dataset tells us the importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.autograd.functional import jacobian\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from L96_model import (\n",
    "    L96,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    "    EulerFwd,\n",
    "    RK2,\n",
    "    RK4,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate L96 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "Forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"synthetic world\" with K=8 and J=32\n",
    "K = 8\n",
    "J = 32\n",
    "W = L96(K, J, F=Forcing)\n",
    "# Get training data for the neural network.\n",
    "\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xy_true):\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(result):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.set_xlabel(\"Shift in score\")\n",
    "    ax.set_ylabel(\"Column\")\n",
    "    ax.set_title(\"Permutation Feature Importance\")\n",
    "\n",
    "    predictors = result[:, 0]\n",
    "    scores = result[:, 1]\n",
    "    y_pos = range(len(predictors))\n",
    "\n",
    "    ax.barh(y_pos, scores)\n",
    "    plt.yticks(y_pos, predictors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "The two-scale Lorenz '96 system is periodic in nature. Hence for a given output feature, the importance of the input feature should gradually increase and then decrease in a circular manner. To confirm this, we test the importance of the first output feature with respect to individual input features. \n",
    "\n",
    "<center>\n",
    "  <img\n",
    "    src=\"https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png\"\n",
    "    width=400\n",
    "  />\n",
    "</center>\n",
    "\n",
    "<span> <center> *Fig. 1: Visualisation of a two-scale Lorenz '96 system with J = 8 and K = 6. Global-scale variables ($X_k$) are updated based on neighbouring variables and on the local-scale variables ($Y_{j,k}$) associated with the corresponding global-scale variable. Local-scale variabless are updated based on neighbouring variables and the associated global-scale variable. The neighbourhood topology of both local and global-scale variables is circular. Image from [Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures - Scientific Figure on ResearchGate.](https://www.researchgate.net/figure/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values_fig1_319201436)* </center> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xy_true[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_true, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = r2_score(model.predict(X_true), y)\n",
    "results = []\n",
    "for column in range(X_true.shape[1]):\n",
    "    # Create a copy of X_test\n",
    "    X_true_copy = X_true.copy()\n",
    "\n",
    "    # Scramble the values of the given predictor\n",
    "    np.random.shuffle(X_true_copy[:, column])\n",
    "\n",
    "    # Calculate the new R2\n",
    "    score = r2_score(model.predict(X_true_copy), y)\n",
    "\n",
    "    # Append the increase in R2 to the list of results\n",
    "    results.append([column, abs(score - baseline)])\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance using Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path\n",
    "PATH = \"networks/network_3_layers_100_epoches.pth\"\n",
    "# Load\n",
    "weights = torch.load(PATH)\n",
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.linear1.weight.dtype)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_ANN()\n",
    "model.load_state_dict(weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = r2_score(model(torch.tensor(X_true)).detach().numpy()[:, feature_index], y)\n",
    "results = []\n",
    "for column in range(X_true.shape[1]):\n",
    "    # Create a copy of X_test\n",
    "    X_true_copy = X_true.copy()\n",
    "\n",
    "    # Scramble the values of the given predictor\n",
    "    np.random.shuffle(X_true_copy[:, column])\n",
    "\n",
    "    # Calculate the new R2\n",
    "    score = r2_score(\n",
    "        model(torch.tensor(X_true_copy)).detach().numpy()[:, feature_index], y\n",
    "    )\n",
    "\n",
    "    # Append the increase in R2 to the list of results\n",
    "    results.append([column, abs(score - baseline)])\n",
    "\n",
    "# Put the results into a pandas dataframe and rank the predictors by score\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce the idea of permutation feature importance. We test the importance of individual input features for the first output feature. We observe:\n",
    "- The first input feature has the highest impact on the first output feature regardless of the model used to evaluate the data.\n",
    "- The last input feature has observable impact on the first output feature showcasing the periodic nature of the two-scale Lorenz '96 system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
