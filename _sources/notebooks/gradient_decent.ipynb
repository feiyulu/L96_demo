{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outline:\n",
    "- Introduce machine learning fundamentals and tools used by the scientific community.\n",
    "- Introduce the linear regression model and gradient descent optimization\n",
    "- Visualize how we can optimize a linear regression model using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundamentals:\n",
    "Machine learning has proven to be a useful tool to the scientific community by being able to solve almost any given problem in recent years. In this notebook, we will be covering the fundamentals of machine learning and demonstrating how we can apply it to model 2D data.\n",
    "\n",
    "Before we can get to modeling data, we need to understand what tools to use and how they work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is [PyTorch](https://github.com/pytorch/pytorch)?\n",
    "\n",
    "In short, PyTorch is a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- A deep learning research platform that provides flexibility and speed\n",
    "\n",
    "To understand and install PyTorch, we recommend going through the [tutorials](https://pytorch.org/tutorials/) and [blogs](https://pytorch.org/blog/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a [Tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)?\n",
    "Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU\n",
    "and more importantly for us - they can keep track of its gradients.\n",
    "\n",
    "To follow along a single blogpost that goes through all concepts, we recommend [this](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having understood how the tools work, we can now use them to model our data. To train any machine learning model, there is a general recipe followed. This recipe can be summarized in three steps:\n",
    "1. Define the model representation.\n",
    "2. Choose a suitable loss function that tells us how far apart our model predictions are from the real data\n",
    "3. Update the model representation using an optimization algorithm.\n",
    "\n",
    "In this notebook, we will be introducing the linear regression model. We will measure loss using the Mean Squared Error (MSE) function and will try to optimize or reduce the loss value using a popular optimization algorithm, gradient descent.\n",
    "\n",
    "To read further on how to train machine learning models, refer to Prof. Fund's [lecture notes](https://github.com/ffund/intro-ml-tss21) or Prof. Hegde's [blogposts](https://chinmayhegde.github.io/dl-notes/notes/lecture03/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest machine learning algorithm is [linear regression](https://en.wikipedia.org/wiki/Linear_regression). We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard's fast.ai [lesson zero](https://www.youtube.com/watch?v=ACU-T9L4_lI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In linear regression, we assume that $y = w_0 + w_1 x $, where $y$ is the expected output.\n",
    "- We look for the $w$ coefficients that give the 'best' prediction ($\\tilde{y}$) for the output. The best prediction is defined by minimizing some cost function. For linear regression, we generally minimize the mean sequare error between the expected output $y$ and the prediction ($\\tilde{y}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "```{image} https://miro.medium.com/v2/resize:fit:1032/1*WswH2fPx0bf_JFRMm8V-HA.gif\n",
    ":alt: nn-output-computation\n",
    ":width: 500px\n",
    "```\n",
    "\n",
    "Image is taken from [here](https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We will learn the parameters $w_0$ and $\\mathbf{w}_1$ of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting packages\n",
    "from IPython.display import Image, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "# Import machine-learning packages\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose the true parameters we want to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([3.0, 2])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Create some data points x and y which lie on the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Documentation for:\n",
    "- [ones()](https://pytorch.org/docs/stable/generated/torch.ones.html)\n",
    "- [uniform_()](https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = torch.ones(n, 2)\n",
    "# Underscore functions in pytorch means replace the value (update)\n",
    "x[:, 1].uniform_(-1.0, 1)\n",
    "\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x @ w + torch.rand(n)  # @ is a matrix product (similar to matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.scatter(x[:, 1], y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_real = torch.as_tensor([-3.0, -5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "If we could find a way to fit our guess for the coefficients the weights ($w_0$ and $\\mathbf{w}_1$), we could use the exact same method for very complicated tasks (as in image recognition). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function $L$ as Mean Square Error loss as:\n",
    "\\begin{equation*}\n",
    "L_{MSE} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - \\tilde y_i)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written in terms of $w_0$ and $w_1$, our **loss function** $L$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{MSE} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - (w_0 + \\mathbf{w_1} \\cdot x_i))^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x @ w_real\n",
    "# Initial mean-squared error\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.scatter(x[:, 1], y, label=\"y\")\n",
    "plt.scatter(x[:, 1], y_hat, label=\"$\\\\tilde{y}$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.Parameter(w_real)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image file\n",
    "with open(\"figs/Gradient_descent2.png\", \"rb\") as f:\n",
    "    image_data = f.read()\n",
    "\n",
    "# Create the HTML code to display the image\n",
    "html = f'<div style=\"text-align:center\"><img src=\"data:image/png;base64,{base64.b64encode(image_data).decode()}\" style=\"max-width:700px;\"/></div>'\n",
    "\n",
    "# Display the HTML code in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for weights ($w_0$, $w_1$) such that they best fit the linear regression line?\n",
    "\n",
    "To know how to change $w_0$ and $w_1$ to reduce the loss, we compute the derivatives (or gradients):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L_{MSE}}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^{n} -2\\cdot [y_i - (w_0 + \\mathbf{w_1}\\cdot x_i)]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L_{MSE}}{\\partial \\mathbf{w_1}} = \\frac{1}{n}\\sum_{i=1}^{n} -2\\cdot [y_i - (w_0 + \\mathbf{w_1}\\cdot x_i)] \\cdot x_i\n",
    "\\end{equation*}\n",
    "\n",
    "Since we know that we can iteratively take little steps down along the gradient to reduce the loss, aka, *gradient descent*, the size of the step is determined by the learning rate ($\\eta$):\n",
    "\n",
    "\\begin{equation*}\n",
    "w_0^{new} = w_0^{current} -   \\eta \\cdot \\frac{\\partial L_{MSE}}{\\partial w_0}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{w_1^{new}} = \\mathbf{w_1^{current}} -  \\eta \\cdot \\frac{\\partial L_{MSE}}{\\partial \\mathbf{w_1}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(lr):\n",
    "    y_hat = x @ w\n",
    "    loss = mse(y, y_hat)\n",
    "    # calculate the gradient of a tensor! It is now stored at w.grad\n",
    "    loss.backward()\n",
    "\n",
    "    # To prevent tracking history and using memory\n",
    "    # (code block where we don't need to track the gradients but only modify the values of tensors)\n",
    "    with torch.no_grad():\n",
    "        # lr is the learning rate. Good learning rate is a key part of Neural Networks.\n",
    "        w.sub_(lr * w.grad)\n",
    "        # We want to zero the gradient before we are re-evaluate it.\n",
    "        w.grad.zero_()\n",
    "\n",
    "    return loss.detach().item(), y_hat.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training Recurrent Neural Networks (RNNs). So, the default action is to accumulate or sum the gradients on every `loss.backward()` call.\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "Explanations about how PyTorch calculates the gradients can be found [here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.1\n",
    "losses = [float(\"inf\")]\n",
    "y_hats = []\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and perform gradient descent\n",
    "for _ in range(epoch):\n",
    "    loss, y_hat = step(lr)\n",
    "    losses.append(loss)\n",
    "    y_hats.append(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Animation inspired by Prof. Fund's pratical [notebooks](https://github.com/ffund/ml-notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), dpi=80)\n",
    "axs[0].scatter(x[:, 1], y, label=\"y\")\n",
    "scatter_yhat = axs[0].scatter(x[:, 1], y_hat, label=\"$\\\\tilde{y}$\")\n",
    "axs[0].set_xlabel(\"$x$\")\n",
    "axs[0].legend(fontsize=7)\n",
    "\n",
    "(line,) = axs[1].plot(range(len(losses)), np.array(losses))\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].set_title(\"Loss vs Iteration\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    axs[0].set_title(\"Loss = %.2f\" % losses[i])\n",
    "    scatter_yhat.set_offsets(np.c_[[], []])\n",
    "    scatter_yhat.set_offsets(np.c_[x[:, 1], y_hats[i]])\n",
    "    line.set_data(np.array(range(i + 1)), np.array(losses[: (i + 1)]))\n",
    "    return scatter_yhat, line\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "animation = FuncAnimation(fig, animate, frames=epoch, interval=100, blit=True)\n",
    "# let animation load\n",
    "time.sleep(1)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have some difficulties running the cell below without importing certain packages. Run the following code in a terminal in `L96M2lines` environment.\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<div style=\"text-align:center;\">{animation.to_html5_video()}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The complete loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(np.array(losses))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "In Deep learning, we use a variation of gradient descent called [mini-batch gradient descent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/). Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
